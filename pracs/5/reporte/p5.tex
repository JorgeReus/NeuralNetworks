% !TeX document-id = {a7325568-63cb-46d1-95d6-777a45054e5a}
% !TeX TXS-program:compile = txs:///pdflatex/[--shell-escape]
\documentclass[6pt]{article}
\usepackage{style}
\begin{document}
\maketitle
\tableofcontents
\section{Introducción}
Un perceptrón multicapa es tipo de red neuronal artifical compuesta de varias neuronas y varias capas, puede resolver problemas que un perceptrón simple puede. El clásico ejemplo es el de la compuerta XOR:
\begin{figure}[h!]
	\caption{Compuerta XOR}
	\centering
	\includegraphics{xor}
\end{figure}
\\
En este caso se necesitan dos fronteras de desición, por ende dos neuron y se necesita otra capa para ``combinar los resultados''. La manera la cual se actualizan los pesos y bías del MLP es usando un algoritmo para propagar los resultados a las neuronas de la capa actual y de las anteriores, es algoritmo es llamado backpropagation, el cual es un algoritmo de minización basado en el descenso en gradiente el cual encuentra los mínimos locales de una función. En esta práctica se usan MLP's para aproximar señales leyendo datos de archivos de texto.
\newpage
\subsection{Modelo}
\begin{figure}[h!]
	\caption{Modelo}
	\centering
	\includegraphics[scale=1]{mlp}
\end{figure}
\subsubsection{Foward Propagation}
$$ a^0 = p$$
$$ a^{m+1} = f^{m+1}(W^{m+1} \cdot a^m + b^{m+1}), \text{m}=0,1,2,3,\dots,M-1$$
$$ a=a^M $$
donde $M$ es el número de capas.\\
\subsubsection{Foward Propagation}
Para poder usar backpropagation se necesita que las funciones de activación en cada capa sean continuas y derivables.
El algoritmo es el siguiente:
\begin{enumerate}
	\item Calcular las sensitividades de cada capa desde la última capa hasta la primera: 
	$$ s^M = -2\dot{F}^M(n^M)(t-a)$$
	$$ s^m = \dot{F}^m(n^m)(W^{m + 1})^T s^{m+1} $$
	\item Actualizar los pesos y bias:
	$$ w^{m}(k+1) = W^m(k)-\alpha s^m(a^m-1)^T $$
	$$ b^{m}(k+1) = b^m(k)-\alpha s^m $$	
\end{enumerate}

\newpage
\section{Diagrama de Flujo}
\begin{figure}[htpb]
	\centering
	\includesvg[width = 400pt, height = 400pt]{diagram}
	\caption{Diagrama de Flujo}
\end{figure}
\newpage
\section{Resultados}
\subsection{Polinomio 1}
\subsection{Inputs}
\begin{multicols}{4}
	\begin{enumerate}
		\item -2.0000
		\item -1.9600
		\item -1.9200
		\item -1.8800
		\item -1.8400
		\item -1.8000
		\item -1.7600
		\item -1.7200
		\item -1.6800
		\item -1.6400
		\item -1.6000
		\item -1.5600
		\item -1.5200
		\item -1.4800
		\item -1.4400
		\item -1.4000
		\item -1.3600
		\item -1.3200
		\item -1.2800
		\item -1.2400
		\item -1.2000
		\item -1.1600
		\item -1.1200
		\item -1.0800
		\item -1.0400
		\item -1.0000
		\item -0.9600
		\item -0.9200
		\item -0.8800
		\item -0.8400
		\item -0.8000
		\item -0.7600
		\item -0.7200
		\item -0.6800
		\item -0.6400
		\item -0.6000
		\item -0.5600
		\item -0.5200
		\item -0.4800
		\item -0.4400
		\item -0.4000
		\item -0.3600
		\item -0.3200
		\item -0.2800
		\item -0.2400
		\item -0.2000
		\item -0.1600
		\item -0.1200
		\item -0.0800
		\item -0.0400
		\item  0
		\item  0.0400
		\item  0.0800
		\item  0.1200
		\item  0.1600
		\item  0.2000
		\item  0.2400
		\item  0.2800
		\item  0.3200
		\item  0.3600
		\item  0.4000
		\item  0.4400
		\item  0.4800
		\item  0.5200
		\item  0.5600
		\item  0.6000
		\item  0.6400
		\item  0.6800
		\item  0.7200
		\item  0.7600
		\item  0.8000
		\item  0.8400
		\item  0.8800
		\item  0.9200
		\item  0.9600
		\item  1.0000
		\item  1.0400
		\item  1.0800
		\item  1.1200
		\item  1.1600
		\item  1.2000
		\item  1.2400
		\item  1.2800
		\item  1.3200
		\item  1.3600
		\item  1.4000
		\item  1.4400
		\item  1.4800
		\item  1.5200
		\item  1.5600
		\item  1.6000
		\item  1.6400
		\item  1.6800
		\item  1.7200
		\item  1.7600
		\item  1.8000
		\item  1.8400
		\item  1.8800
		\item  1.9200
		\item  1.9600
		\item  2.0000
	\end{enumerate}
\end{multicols}
\subsection{Targets}
\begin{multicols}{4}
\begin{enumerate}
	\item 1.0000 
	\item 1.2487 
	\item 1.4818 
	\item 1.6845 
	\item 1.8443 
	\item 1.9511 
	\item 1.9980 
	\item 1.9823 
	\item 1.9048 
	\item 1.7705 
	\item 1.5878 
	\item 1.3681 
	\item 1.1253 
	\item 0.8747 
	\item 0.6319 
	\item 0.4122 
	\item 0.2295 
	\item 0.0952 
	\item 0.0177 
	\item 0.0020 
	\item 0.0489 
	\item 0.1557 
	\item 0.3155 
	\item 0.5182 
	\item 0.7513 
	\item 1.0000 
	\item 1.2487 
	\item 1.4818 
	\item 1.6845 
	\item 1.8443 
	\item 1.9511 
	\item 1.9980 
	\item 1.9823 
	\item 1.9048 
	\item 1.7705 
	\item 1.5878 
	\item 1.3681 
	\item 1.1253 
	\item 0.8747 
	\item 0.6319 
	\item 0.4122 
	\item 0.2295 
	\item 0.0952 
	\item 0.0177 
	\item 0.0020 
	\item 0.0489 
	\item 0.1557 
	\item 0.3155 
	\item 0.5182 
	\item 0.7513 
	\item 1.0000 
	\item 1.2487 
	\item 1.4818 
	\item 1.6845 
	\item 1.8443 
	\item 1.9511 
	\item 1.9980 
	\item 1.9823 
	\item 1.9048 
	\item 1.7705 
	\item 1.5878 
	\item 1.3681 
	\item 1.1253 
	\item 0.8747 
	\item 0.6319 
	\item 0.4122 
	\item 0.2295 
	\item 0.0952 
	\item 0.0177 
	\item 0.0020 
	\item 0.0489 
	\item 0.1557 
	\item 0.3155 
	\item 0.5182 
	\item 0.7513 
	\item 1.0000 
	\item 1.2487 
	\item 1.4818 
	\item 1.6845 
	\item 1.8443 
	\item 1.9511 
	\item 1.9980 
	\item 1.9823 
	\item 1.9048 
	\item 1.7705 
	\item 1.5878 
	\item 1.3681 
	\item 1.1253 
	\item 0.8747 
	\item 0.6319 
	\item 0.4122 
	\item 0.2295 
	\item 0.0952 
	\item 0.0177 
	\item 0.0020 
	\item 0.0489 
	\item 0.1557 
	\item 0.3155 
	\item 0.5182 
	\item 0.7513 
	\item 1.0000
\end{enumerate}
\end{multicols}
\subsubsection{Datos}
\[V1=
\begin{bmatrix}
1\ 16\ 10\ 1
\end{bmatrix}\]

\[V2=
\begin{bmatrix}
3\ 2\ 1
\end{bmatrix}\]
epochmax = 10000\\
Múltiplo para las épocas de validación = 500\\
numval = 7\\
alpha=.0701\\
error de validación = .0000000000000001\\
Configuración: 80-15-15
\subsection{Resultado}
\subsection{Imágenes}
\begin{figure}[htpb]
	\centering
	\includesvg[width = 500pt, height = 500pt]{pol1/1}
	\caption{Gráfica 1.1}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includesvg[width = 500pt, height = 500pt]{pol1/2}
	\caption{Gráfica 1.2}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includesvg[width = 500pt, height = 500pt]{pol1/3}
	\caption{Gráfica 1.3}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includesvg[width = 500pt, height = 500pt]{pol1/4}
	\caption{Gráfica 1.4}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includesvg[width = 500pt, height = 500pt]{pol1/5}
	\caption{Gráfica 1.5}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includesvg[width = 500pt, height = 500pt]{pol1/6}
	\caption{Gráfica 1.6}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includesvg[width = 500pt, height = 500pt]{pol1/7}
	\caption{Gráfica 1.7}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includesvg[width = 500pt, height = 500pt]{pol1/8}
	\caption{Gráfica 1.8}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includesvg[width = 500pt, height = 500pt]{pol1/9}
	\caption{Gráfica 1.9}
\end{figure}
\newpage
\subsection{Polinomio 2}
\subsection{Inputs}
\begin{multicols}{4}
	\begin{enumerate}
		\item 4.0000
		\item 3.9667
		\item 3.9333
		\item 3.9000
		\item 3.8667
		\item 3.8333
		\item 3.8000
		\item 3.7667
		\item 3.7333
		\item 3.7000
		\item 3.6667
		\item 3.6333
		\item 3.6000
		\item 3.5667
		\item 3.5333
		\item 3.5000
		\item 3.4667
		\item 3.4333
		\item 3.4000
		\item 3.3667
		\item 3.3333
		\item 3.3000
		\item 3.2667
		\item 3.2333
		\item 3.2000
		\item 3.1667
		\item 3.1333
		\item 3.1000
		\item 3.0667
		\item 3.0333
		\item 3.0000
		\item 2.9667
		\item 2.9333
		\item 2.9000
		\item 2.8667
		\item 2.8333
		\item 2.8000
		\item 2.7667
		\item 2.7333
		\item 2.7000
		\item 2.6667
		\item 2.6333
		\item 2.6000
		\item 2.5667
		\item 2.5333
		\item 2.5000
		\item 2.4667
		\item 2.4333
		\item 2.4000
		\item 2.3667
		\item 2.3333
		\item 2.3000
		\item 2.2667
		\item 2.2333
		\item 2.2000
		\item 2.1667
		\item 2.1333
		\item 2.1000
		\item 2.0667
		\item 2.0333
		\item 2.0000
		\item 1.9667
		\item 1.9333
		\item 1.9000
		\item 1.8667
		\item 1.8333
		\item 1.8000
		\item 1.7667
		\item 1.7333
		\item 1.7000
		\item 1.6667
		\item 1.6333
		\item 1.6000
		\item 1.5667
		\item 1.5333
		\item 1.5000
		\item 1.4667
		\item 1.4333
		\item 1.4000
		\item 1.3667
		\item 1.3333
		\item 1.3000
		\item 1.2667
		\item 1.2333
		\item 1.2000
		\item 1.1667
		\item 1.1333
		\item 1.1000
		\item 1.0667
		\item 1.0333
		\item 1.0000
		\item 0.9667
		\item 0.9333
		\item 0.9000
		\item 0.8667
		\item 0.8333
		\item 0.8000
		\item 0.7667
		\item 0.7333
		\item 0.7000
		\item 0.6667
		\item 0.6333
		\item 0.6000
		\item 0.5667
		\item 0.5333
		\item 0.5000
		\item 0.4667
		\item 0.4333
		\item 0.4000
		\item 0.3667
		\item 0.3333
		\item 0.3000
		\item 0.2667
		\item 0.2333
		\item 0.2000
		\item 0.1667
		\item 0.1333
		\item 0.1000
		\item 0.0667
		\item 0.0333
		\item 0.0000
		\item -0.0333
		\item -0.0667
		\item -0.1000
		\item -0.1333
		\item -0.1667
		\item -0.2000
		\item -0.2333
		\item -0.2667
		\item -0.3000
		\item -0.3333
		\item -0.3667
		\item -0.4000
		\item -0.4333
		\item -0.4667
		\item -0.5000
		\item -0.5333
		\item -0.5667
		\item -0.6000
		\item -0.6333
		\item -0.6667
		\item -0.7000
		\item -0.7333
		\item -0.7667
		\item -0.8000
		\item -0.8333
		\item -0.8667
		\item -0.9000
		\item -0.9333
		\item -0.9667
		\item -1.0000
		\item -1.0333
		\item -1.0667
		\item -1.1000
		\item -1.1333
		\item -1.1667
		\item -1.2000
		\item -1.2333
		\item -1.2667
		\item -1.3000
		\item -1.3333
		\item -1.3667
		\item -1.4000
		\item -1.4333
		\item -1.4667
		\item -1.5000
		\item -1.5333
		\item -1.5667
		\item -1.6000
		\item -1.6333
		\item -1.6667
		\item -1.7000
		\item -1.7333
		\item -1.7667
		\item -1.8000
		\item -1.8333
		\item -1.8667
		\item -1.9000
		\item -1.9333
		\item -1.9667
		\item -2.0000
		\item -2.0333
		\item -2.0667
		\item -2.1000
		\item -2.1333
		\item -2.1667
		\item -2.2000
		\item -2.2333
		\item -2.2667
		\item -2.3000
		\item -2.3333
		\item -2.3667
		\item -2.4000
		\item -2.4333
		\item -2.4667
		\item -2.5000
		\item -2.5333
		\item -2.5667
		\item -2.6000
		\item -2.6333
		\item -2.6667
		\item -2.7000
		\item -2.7333
		\item -2.7667
		\item -2.8000
		\item -2.8333
		\item -2.8667
		\item -2.9000
		\item -2.9333
		\item -2.9667
		\item -3.0000
		\item -3.0333
		\item -3.0667
		\item -3.1000
		\item -3.1333
		\item -3.1667
		\item -3.2000
		\item -3.2333
		\item -3.2667
		\item -3.3000
		\item -3.3333
		\item -3.3667
		\item -3.4000
		\item -3.4333
		\item -3.4667
		\item -3.5000
		\item -3.5333
		\item -3.5667
		\item -3.6000
		\item -3.6333
		\item -3.6667
		\item -3.7000
		\item -3.7333
		\item -3.7667
		\item -3.8000
		\item -3.8333
		\item -3.8667
		\item -3.9000
		\item -3.9333
		\item -3.9667
		\item -4.0000
	\end{enumerate}
\end{multicols}
\subsection{Targets}
\begin{multicols}{4}
	\begin{enumerate}
	\item -0.01292
	\item -0.01451
	\item -0.01628
	\item -0.01825
	\item -0.02043
	\item -0.02284
	\item -0.02550
	\item -0.02844
	\item -0.03167
	\item -0.03523
	\item -0.03914
	\item -0.04342
	\item -0.04812
	\item -0.05325
	\item -0.05886
	\item -0.06497
	\item -0.07162
	\item -0.07886
	\item -0.08671
	\item -0.09522
	\item -0.10442
	\item -0.11437
	\item -0.12510
	\item -0.13666
	\item -0.14909
	\item -0.16243
	\item -0.17674
	\item -0.19204
	\item -0.20839
	\item -0.22583
	\item -0.24440
	\item -0.26413
	\item -0.28507
	\item -0.30725
	\item -0.33070
	\item -0.35544
	\item -0.38150
	\item -0.40891
	\item -0.43767
	\item -0.46778
	\item -0.49926
	\item -0.53210
	\item -0.56628
	\item -0.60178
	\item -0.63858
	\item -0.67663
	\item -0.71589
	\item -0.75630
	\item -0.79779
	\item -0.84028
	\item -0.88368
	\item -0.92790
	\item -0.97281
	\item -1.01829
	\item -1.06421
	\item -1.11042
	\item -1.15676
	\item -1.20305
	\item -1.24912
	\item -1.29478
	\item -1.33982
	\item -1.38403
	\item -1.42719
	\item -1.46909
	\item -1.50947
	\item -1.54811
	\item -1.58477
	\item -1.61920
	\item -1.65116
	\item -1.68040
	\item -1.70668
	\item -1.72976
	\item -1.74941
	\item -1.76541
	\item -1.77754
	\item -1.78559
	\item -1.78938
	\item -1.78873
	\item -1.78348
	\item -1.77349
	\item -1.75865
	\item -1.73885
	\item -1.71402
	\item -1.68412
	\item -1.64912
	\item -1.60902
	\item -1.56387
	\item -1.51372
	\item -1.45866
	\item -1.39883
	\item -1.33437
	\item -1.26546
	\item -1.19232
	\item -1.11519
	\item -1.03433
	\item -0.95005
	\item -0.86267
	\item -0.77252
	\item -0.67999
	\item -0.58546
	\item -0.48934
	\item -0.39205
	\item -0.29402
	\item -0.19569
	\item -0.09754
	\item 0.00000
	\item 0.09646
	\item 0.19138
	\item 0.28432
	\item 0.37483
	\item 0.46247
	\item 0.54683
	\item 0.62751
	\item 0.70412
	\item 0.77632
	\item 0.84375
	\item 0.90613
	\item 0.96317
	\item 1.01463
	\item 1.06030
	\item 1.10000
	\item 1.13359
	\item 1.16097
	\item 1.18207
	\item 1.19687
	\item 1.20536
	\item 1.20760
	\item 1.20367
	\item 1.19368
	\item 1.17779
	\item 1.15617
	\item 1.12905
	\item 1.09666
	\item 1.05928
	\item 1.01720
	\item 0.97075
	\item 0.92025
	\item 0.86605
	\item 0.80854
	\item 0.74809
	\item 0.68508
	\item 0.61990
	\item 0.55296
	\item 0.48465
	\item 0.41536
	\item 0.34547
	\item 0.27537
	\item 0.20543
	\item 0.13599
	\item 0.06741
	\item 0.00000
	\item -0.06593
	\item -0.13009
	\item -0.19222
	\item -0.25207
	\item -0.30943
	\item -0.36409
	\item -0.41589
	\item -0.46467
	\item -0.51031
	\item -0.55272
	\item -0.59180
	\item -0.62752
	\item -0.65983
	\item -0.68874
	\item -0.71424
	\item -0.73636
	\item -0.75517
	\item -0.77072
	\item -0.78309
	\item -0.79239
	\item -0.79871
	\item -0.80218
	\item -0.80293
	\item -0.80109
	\item -0.79682
	\item -0.79027
	\item -0.78158
	\item -0.77093
	\item -0.75846
	\item -0.74434
	\item -0.72874
	\item -0.71180
	\item -0.69370
	\item -0.67457
	\item -0.65456
	\item -0.63383
	\item -0.61251
	\item -0.59073
	\item -0.56861
	\item -0.54628
	\item -0.52384
	\item -0.50140
	\item -0.47905
	\item -0.45688
	\item -0.43498
	\item -0.41341
	\item -0.39224
	\item -0.37153
	\item -0.35132
	\item -0.33168
	\item -0.31262
	\item -0.29419
	\item -0.27640
	\item -0.25928
	\item -0.24285
	\item -0.22710
	\item -0.21205
	\item -0.19770
	\item -0.18405
	\item -0.17108
	\item -0.15879
	\item -0.14717
	\item -0.13619
	\item -0.12586
	\item -0.11614
	\item -0.10702
	\item -0.09847
	\item -0.09048
	\item -0.08302
	\item -0.07607
	\item -0.06961
	\item -0.06360
	\item -0.05804
	\item -0.05288
	\item -0.04812
	\item -0.04373
	\item -0.03969
	\item -0.03597
	\item -0.03256
	\item -0.02943
	\item -0.02656
	\item -0.02395
	\item -0.02156
	\item -0.01938
	\item -0.01741
	\item -0.01561
	\item -0.01398
	\item -0.01250
	\item -0.01117
	\item -0.00996
	\end{enumerate}
\end{multicols}
\subsubsection{Datos}
\[V1=
\begin{bmatrix}
11\ 16\ 10\ 1
\end{bmatrix}\]

\[V2=
\begin{bmatrix}
3\ 2\ 1
\end{bmatrix}\]
epochmax = 2200\\
alpha=.0103\\
Múltiplo para las épocas de validación = 500\\
numval = 7\\
error de validación = .000000000001\\
Configuración: 80-10-10
\subsection{Resultado}
\subsection{Imágenes}
\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{pol2/1}
	\caption{Gráfica 1.1}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{pol2/2}
	\caption{Gráfica 1.2}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{pol2/3}
	\caption{Gráfica 1.3}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{pol2/4}
	\caption{Gráfica 1.4}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{pol2/5}
	\caption{Gráfica 1.5}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{pol2/6}
	\caption{Gráfica 1.6}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{pol2/7}
	\caption{Gráfica 1.7}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{pol2/8}
	\caption{Gráfica 1.8}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{pol1/9}
	\caption{Gráfica 1.9}
\end{figure}
\newpage
\subsection{Polinomio 3}
\subsection{Inputs}
\begin{multicols}{4}
	\begin{enumerate}
		\item -3.5
		\item -3.49
		\item -3.48
		\item -3.47
		\item -3.46
		\item -3.45
		\item -3.44
		\item -3.43
		\item -3.42
		\item -3.41
		\item -3.4
		\item -3.39
		\item -3.38
		\item -3.37
		\item -3.36
		\item -3.35
		\item -3.34
		\item -3.33
		\item -3.32
		\item -3.31
		\item -3.3
		\item -3.29
		\item -3.28
		\item -3.27
		\item -3.26
		\item -3.25
		\item -3.24
		\item -3.23
		\item -3.22
		\item -3.21
		\item -3.2
		\item -3.19
		\item -3.18
		\item -3.17
		\item -3.16
		\item -3.15
		\item -3.14
		\item -3.13
		\item -3.12
		\item -3.11
		\item -3.1
		\item -3.09
		\item -3.08
		\item -3.07
		\item -3.06
		\item -3.05
		\item -3.04
		\item -3.03
		\item -3.02
		\item -3.01
		\item -3
		\item -2.99
		\item -2.98
		\item -2.97
		\item -2.96
		\item -2.95
		\item -2.94
		\item -2.93
		\item -2.92
		\item -2.91
		\item -2.9
		\item -2.89
		\item -2.88
		\item -2.87
		\item -2.86
		\item -2.85
		\item -2.84
		\item -2.83
		\item -2.82
		\item -2.81
		\item -2.8
		\item -2.79
		\item -2.78
		\item -2.77
		\item -2.76
		\item -2.75
		\item -2.74
		\item -2.73
		\item -2.72
		\item -2.71
		\item -2.7
		\item -2.69
		\item -2.68
		\item -2.67
		\item -2.66
		\item -2.65
		\item -2.64
		\item -2.63
		\item -2.62
		\item -2.61
		\item -2.6
		\item -2.59
		\item -2.58
		\item -2.57
		\item -2.56
		\item -2.55
		\item -2.54
		\item -2.53
		\item -2.52
		\item -2.51
		\item -2.5
		\item -2.49
		\item -2.48
		\item -2.47
		\item -2.46
		\item -2.45
		\item -2.44
		\item -2.43
		\item -2.42
		\item -2.41
		\item -2.4
		\item -2.39
		\item -2.38
		\item -2.37
		\item -2.36
		\item -2.35
		\item -2.34
		\item -2.33
		\item -2.32
		\item -2.31
		\item -2.3
		\item -2.29
		\item -2.28
		\item -2.27
		\item -2.26
		\item -2.25
		\item -2.24
		\item -2.23
		\item -2.22
		\item -2.21
		\item -2.2
		\item -2.19
		\item -2.18
		\item -2.17
		\item -2.16
		\item -2.15
		\item -2.14
		\item -2.13
		\item -2.12
		\item -2.11
		\item -2.1
		\item -2.09
		\item -2.08
		\item -2.07
		\item -2.06
		\item -2.05
		\item -2.04
		\item -2.03
		\item -2.02
		\item -2.01
		\item -2
		\item -1.99
		\item -1.98
		\item -1.97
		\item -1.96
		\item -1.95
		\item -1.94
		\item -1.93
		\item -1.92
		\item -1.91
		\item -1.9
		\item -1.89
		\item -1.88
		\item -1.87
		\item -1.86
		\item -1.85
		\item -1.84
		\item -1.83
		\item -1.82
		\item -1.81
		\item -1.8
		\item -1.79
		\item -1.78
		\item -1.77
		\item -1.76
		\item -1.75
		\item -1.74
		\item -1.73
		\item -1.72
		\item -1.71
		\item -1.7
		\item -1.69
		\item -1.68
		\item -1.67
		\item -1.66
		\item -1.65
		\item -1.64
		\item -1.63
		\item -1.62
		\item -1.61
		\item -1.6
		\item -1.59
		\item -1.58
		\item -1.57
		\item -1.56
		\item -1.55
		\item -1.54
		\item -1.53
		\item -1.52
		\item -1.51
		\item -1.5
		\item -1.49
		\item -1.48
		\item -1.47
		\item -1.46
		\item -1.45
		\item -1.44
		\item -1.43
		\item -1.42
		\item -1.41
		\item -1.4
		\item -1.39
		\item -1.38
		\item -1.37
		\item -1.36
		\item -1.35
		\item -1.34
		\item -1.33
		\item -1.32
		\item -1.31
		\item -1.3
		\item -1.29
		\item -1.28
		\item -1.27
		\item -1.26
		\item -1.25
		\item -1.24
		\item -1.23
		\item -1.22
		\item -1.21
		\item -1.2
		\item -1.19
		\item -1.18
		\item -1.17
		\item -1.16
		\item -1.15
		\item -1.14
		\item -1.13
		\item -1.12
		\item -1.11
		\item -1.1
		\item -1.09
		\item -1.08
		\item -1.07
		\item -1.06
		\item -1.05
		\item -1.04
		\item -1.03
		\item -1.02
		\item -1.01
		\item -1
		\item -0.99
		\item -0.98
		\item -0.97
		\item -0.96
		\item -0.95
		\item -0.94
		\item -0.93
		\item -0.92
		\item -0.91
		\item -0.9
		\item -0.89
		\item -0.88
		\item -0.87
		\item -0.86
		\item -0.85
		\item -0.84
		\item -0.83
		\item -0.82
		\item -0.81
		\item -0.8
		\item -0.79
		\item -0.78
		\item -0.77
		\item -0.76
		\item -0.75
		\item -0.74
		\item -0.73
		\item -0.72
		\item -0.71
		\item -0.7
		\item -0.69
		\item -0.68
		\item -0.67
		\item -0.66
		\item -0.65
		\item -0.64
		\item -0.63
		\item -0.62
		\item -0.61
		\item -0.6
		\item -0.59
		\item -0.58
		\item -0.57
		\item -0.56
		\item -0.55
		\item -0.54
		\item -0.53
		\item -0.52
		\item -0.51
		\item -0.5
		\item -0.49
		\item -0.48
		\item -0.47
		\item -0.46
		\item -0.45
		\item -0.44
		\item -0.43
		\item -0.42
		\item -0.41
		\item -0.4
		\item -0.39
		\item -0.38
		\item -0.37
		\item -0.36
		\item -0.35
		\item -0.34
		\item -0.33
		\item -0.32
		\item -0.31
		\item -0.3
		\item -0.29
		\item -0.28
		\item -0.27
		\item -0.26
		\item -0.25
		\item -0.24
		\item -0.23
		\item -0.22
		\item -0.21
		\item -0.2
		\item -0.19
		\item -0.18
		\item -0.17
		\item -0.16
		\item -0.15
		\item -0.14
		\item -0.13
		\item -0.12
		\item -0.11
		\item -0.1
		\item -0.09
		\item -0.08
		\item -0.07
		\item -0.06
		\item -0.05
		\item -0.04
		\item -0.03
		\item -0.02
		\item -0.01
		\item 0
		\item 0.01
		\item 0.02
		\item 0.03
		\item 0.04
		\item 0.05
		\item 0.06
		\item 0.07
		\item 0.08
		\item 0.09
		\item 0.1
		\item 0.11
		\item 0.12
		\item 0.13
		\item 0.14
		\item 0.15
		\item 0.16
		\item 0.17
		\item 0.18
		\item 0.19
		\item 0.2
		\item 0.21
		\item 0.22
		\item 0.23
		\item 0.24
		\item 0.25
		\item 0.26
		\item 0.27
		\item 0.28
		\item 0.29
		\item 0.3
		\item 0.31
		\item 0.32
		\item 0.33
		\item 0.34
		\item 0.35
		\item 0.36
		\item 0.37
		\item 0.38
		\item 0.39
		\item 0.4
		\item 0.41
		\item 0.42
		\item 0.43
		\item 0.44
		\item 0.45
		\item 0.46
		\item 0.47
		\item 0.48
		\item 0.49
		\item 0.5
		\item 0.51
		\item 0.52
		\item 0.53
		\item 0.54
		\item 0.55
		\item 0.56
		\item 0.57
		\item 0.58
		\item 0.59
		\item 0.6
		\item 0.61
		\item 0.62
		\item 0.63
		\item 0.64
		\item 0.65
		\item 0.66
		\item 0.67
		\item 0.68
		\item 0.69
		\item 0.7
		\item 0.71
		\item 0.72
		\item 0.73
		\item 0.74
		\item 0.75
		\item 0.76
		\item 0.77
		\item 0.78
		\item 0.79
		\item 0.8
		\item 0.81
		\item 0.82
		\item 0.83
		\item 0.84
		\item 0.85
		\item 0.86
		\item 0.87
		\item 0.88
		\item 0.89
		\item 0.9
		\item 0.91
		\item 0.92
		\item 0.93
		\item 0.94
		\item 0.95
		\item 0.96
		\item 0.97
		\item 0.98
		\item 0.99
		\item 1
		\item 1.01
		\item 1.02
		\item 1.03
		\item 1.04
		\item 1.05
		\item 1.06
		\item 1.07
		\item 1.08
		\item 1.09
		\item 1.1
		\item 1.11
		\item 1.12
		\item 1.13
		\item 1.14
		\item 1.15
		\item 1.16
		\item 1.17
		\item 1.18
		\item 1.19
		\item 1.2
		\item 1.21
		\item 1.22
		\item 1.23
		\item 1.24
		\item 1.25
		\item 1.26
		\item 1.27
		\item 1.28
		\item 1.29
		\item 1.3
		\item 1.31
		\item 1.32
		\item 1.33
		\item 1.34
		\item 1.35
		\item 1.36
		\item 1.37
		\item 1.38
		\item 1.39
		\item 1.4
		\item 1.41
		\item 1.42
		\item 1.43
		\item 1.44
		\item 1.45
		\item 1.46
		\item 1.47
		\item 1.48
		\item 1.49
		\item 1.5
		\item 1.51
		\item 1.52
		\item 1.53
		\item 1.54
		\item 1.55
		\item 1.56
		\item 1.57
		\item 1.58
		\item 1.59
		\item 1.6
		\item 1.61
		\item 1.62
		\item 1.63
		\item 1.64
		\item 1.65
		\item 1.66
		\item 1.67
		\item 1.68
		\item 1.69
		\item 1.7
		\item 1.71
		\item 1.72
		\item 1.73
		\item 1.74
		\item 1.75
		\item 1.76
		\item 1.77
		\item 1.78
		\item 1.79
		\item 1.8
		\item 1.81
		\item 1.82
		\item 1.83
		\item 1.84
		\item 1.85
		\item 1.86
		\item 1.87
		\item 1.88
		\item 1.89
		\item 1.9
		\item 1.91
		\item 1.92
		\item 1.93
		\item 1.94
		\item 1.95
		\item 1.96
		\item 1.97
		\item 1.98
		\item 1.99
		\item 2
		\item 2.01
		\item 2.02
		\item 2.03
		\item 2.04
		\item 2.05
		\item 2.06
		\item 2.07
		\item 2.08
		\item 2.09
		\item 2.1
		\item 2.11
		\item 2.12
		\item 2.13
		\item 2.14
		\item 2.15
		\item 2.16
		\item 2.17
		\item 2.18
		\item 2.19
		\item 2.2
		\item 2.21
		\item 2.22
		\item 2.23
		\item 2.24
		\item 2.25
		\item 2.26
		\item 2.27
		\item 2.28
		\item 2.29
		\item 2.3
		\item 2.31
		\item 2.32
		\item 2.33
		\item 2.34
		\item 2.35
		\item 2.36
		\item 2.37
		\item 2.38
		\item 2.39
		\item 2.4
		\item 2.41
		\item 2.42
		\item 2.43
		\item 2.44
		\item 2.45
		\item 2.46
		\item 2.47
		\item 2.48
		\item 2.49
		\item 2.5
		\item 2.51
		\item 2.52
		\item 2.53
		\item 2.54
		\item 2.55
		\item 2.56
		\item 2.57
		\item 2.58
		\item 2.59
		\item 2.6
		\item 2.61
		\item 2.62
		\item 2.63
		\item 2.64
		\item 2.65
		\item 2.66
		\item 2.67
		\item 2.68
		\item 2.69
		\item 2.7
		\item 2.71
		\item 2.72
		\item 2.73
		\item 2.74
		\item 2.75
		\item 2.76
		\item 2.77
		\item 2.78
		\item 2.79
		\item 2.8
		\item 2.81
		\item 2.82
		\item 2.83
		\item 2.84
		\item 2.85
		\item 2.86
		\item 2.87
		\item 2.88
		\item 2.89
		\item 2.9
		\item 2.91
		\item 2.92
		\item 2.93
		\item 2.94
		\item 2.95
		\item 2.96
		\item 2.97
		\item 2.98
		\item 2.99
		\item 3
		\item 3.01
		\item 3.02
		\item 3.03
		\item 3.04
		\item 3.05
		\item 3.06
		\item 3.07
		\item 3.08
		\item 3.09
		\item 3.1
		\item 3.11
		\item 3.12
		\item 3.13
		\item 3.14
		\item 3.15
		\item 3.16
		\item 3.17
		\item 3.18
		\item 3.19
		\item 3.2
		\item 3.21
		\item 3.22
		\item 3.23
		\item 3.24
		\item 3.25
		\item 3.26
		\item 3.27
		\item 3.28
		\item 3.29
		\item 3.3
		\item 3.31
		\item 3.32
		\item 3.33
		\item 3.34
		\item 3.35
		\item 3.36
		\item 3.37
		\item 3.38
		\item 3.39
		\item 3.4
		\item 3.41
		\item 3.42
		\item 3.43
		\item 3.44
		\item 3.45
		\item 3.46
		\item 3.47
		\item 3.48
		\item 3.49
		\item 3.5
	\end{enumerate}
\end{multicols}
\subsection{Targets}
\begin{multicols}{4}
	\begin{enumerate}
		\item 1
		\item 0.928632986
		\item 0.859165985
		\item 0.791568247
		\item 0.725809023
		\item 0.661858371
		\item 0.599686352
		\item 0.539262216
		\item 0.480557639
		\item 0.423544301
		\item 0.368191451
		\item 0.314472386
		\item 0.262357974
		\item 0.211821512
		\item 0.162833868
		\item 0.115369147
		\item 0.069400646
		\item 0.024900852
		\item -0.018156939
		\item -0.059797003
		\item -0.100046044
		\item -0.138929956
		\item -0.176473016
		\item -0.212701118
		\item -0.247637729
		\item -0.281307125
		\item -0.313734391
		\item -0.344942186
		\item -0.374954786
		\item -0.403794847
		\item -0.431485838
		\item -0.458049606
		\item -0.483508809
		\item -0.507885297
		\item -0.531201726
		\item -0.553479136
		\item -0.574738566
		\item -0.595001056
		\item -0.614288454
		\item -0.632620181
		\item -0.650016467
		\item -0.666498352
		\item -0.682084447
		\item -0.696794982
		\item -0.710649379
		\item -0.723665439
		\item -0.735864202
		\item -0.747261853
		\item -0.757877812
		\item -0.767729881
		\item -0.776836673
		\item -0.78521518
		\item -0.792882396
		\item -0.799855314
		\item -0.806151737
		\item -0.811788658
		\item -0.816780643
		\item -0.821145494
		\item -0.824899396
		\item -0.828056913
		\item -0.83063423
		\item -0.832646723
		\item -0.834108956
		\item -0.835037114
		\item -0.835444953
		\item -0.83534704
		\item -0.834757939
		\item -0.833691406
		\item -0.8321612
		\item -0.830181885
		\item -0.827765599
		\item -0.824926099
		\item -0.821677142
		\item -0.818030865
		\item -0.814001024
		\item -0.809598141
		\item -0.80483597
		\item -0.799725841
		\item -0.794279893
		\item -0.788510262
		\item -0.782428278
		\item -0.77604527
		\item -0.769371758
		\item -0.762419879
		\item -0.755199344
		\item -0.747721481
		\item -0.739996003
		\item -0.732033427
		\item -0.723845083
		\item -0.715439062
		\item -0.706825886
		\item -0.698015263
		\item -0.689016905
		\item -0.679839713
		\item -0.670491778
		\item -0.660983621
		\item -0.651323333
		\item -0.641519007
		\item -0.631579544
		\item -0.621513844
		\item -0.611329192
		\item -0.601033678
		\item -0.590634587
		\item -0.580140818
		\item -0.569559655
		\item -0.558897572
		\item -0.548161851
		\item -0.537360584
		\item -0.526500246
		\item -0.515587309
		\item -0.504628247
		\item -0.493630344
		\item -0.482599263
		\item -0.471541479
		\item -0.460463464
		\item -0.449370884
		\item -0.438268593
		\item -0.427163875
		\item -0.416061584
		\item -0.404966576
		\item -0.393885324
		\item -0.382822685
		\item -0.371782703
		\item -0.360771043
		\item -0.34979337
		\item -0.33885292
		\item -0.327954549
		\item -0.317103111
		\item -0.306303463
		\item -0.295558841
		\item -0.284873291
		\item -0.274251668
		\item -0.263697208
		\item -0.253213959
		\item -0.242805157
		\item -0.232474848
		\item -0.222226268
		\item -0.212062655
		\item -0.201987245
		\item -0.192004085
		\item -0.182114792
		\item -0.172322604
		\item -0.162630757
		\item -0.153042489
		\item -0.143559417
		\item -0.134183969
		\item -0.124919383
		\item -0.115768085
		\item -0.106731694
		\item -0.097812638
		\item -0.089012535
		\item -0.080333813
		\item -0.071778899
		\item -0.063348603
		\item -0.055045352
		\item -0.046870764
		\item -0.038826459
		\item -0.030913244
		\item -0.02313274
		\item -0.015486563
		\item -0.007975523
		\item -0.000601239
		\item 0.006634671
		\item 0.013732207
		\item 0.020690559
		\item 0.02750811
		\item 0.03418405
		\item 0.04071838
		\item 0.04710948
		\item 0.05335816
		\item 0.059461992
		\item 0.065422595
		\item 0.071237541
		\item 0.076908449
		\item 0.0824337
		\item 0.087813294
		\item 0.09304804
		\item 0.09813632
		\item 0.103078944
		\item 0.107876719
		\item 0.112528838
		\item 0.1170353
		\item 0.121396105
		\item 0.125612872
		\item 0.12968479
		\item 0.133612671
		\item 0.137396513
		\item 0.141037126
		\item 0.144535318
		\item 0.147890282
		\item 0.151104443
		\item 0.154177803
		\item 0.157110362
		\item 0.159903737
		\item 0.162557929
		\item 0.165075365
		\item 0.167454428
		\item 0.169698353
		\item 0.171807141
		\item 0.1737816
		\item 0.175622542
		\item 0.177332391
		\item 0.178910341
		\item 0.180359627
		\item 0.18167944
		\item 0.182872208
		\item 0.18393874
		\item 0.184880654
		\item 0.18569957
		\item 0.186395486
		\item 0.186970831
		\item 0.187427222
		\item 0.18776547
		\item 0.187987192
		\item 0.188093198
		\item 0.188086724
		\item 0.187966962
		\item 0.187737957
		\item 0.187399709
		\item 0.186953837
		\item 0.186402769
		\item 0.185747313
		\item 0.184989897
		\item 0.18413214
		\item 0.183174851
		\item 0.182120457
		\item 0.180970577
		\item 0.179726829
		\item 0.178391641
		\item 0.176965822
		\item 0.175451799
		\item 0.173851192
		\item 0.172165618
		\item 0.170397506
		\item 0.168547663
		\item 0.166619328
		\item 0.164613309
		\item 0.162531225
		\item 0.160376313
		\item 0.15815019
		\item 0.155853667
		\item 0.15348917
		\item 0.151058319
		\item 0.148564349
		\item 0.146008071
		\item 0.143391102
		\item 0.140715871
		\item 0.137984805
		\item 0.135199522
		\item 0.13236164
		\item 0.129472779
		\item 0.126535365
		\item 0.123551827
		\item 0.120523782
		\item 0.11745285
		\item 0.114340648
		\item 0.111189604
		\item 0.108002146
		\item 0.104779892
		\item 0.101523652
		\item 0.098236662
		\item 0.09492054
		\item 0.091577715
		\item 0.088208995
		\item 0.084815999
		\item 0.081402773
		\item 0.077968507
		\item 0.074517249
		\item 0.071049806
		\item 0.067567797
		\item 0.064073651
		\item 0.060569794
		\item 0.057056226
		\item 0.053536994
		\item 0.050012098
		\item 0.046483964
		\item 0.042954213
		\item 0.03942527
		\item 0.035897946
		\item 0.032374668
		\item 0.028857055
		\item 0.025347533
		\item 0.021846104
		\item 0.018355194
		\item 0.014876422
		\item 0.011412216
		\item 0.007963385
		\item 0.004532356
		\item 0.00111913
		\item -0.002273056
		\item -0.005642586
		\item -0.008989457
		\item -0.012311243
		\item -0.015606325
		\item -0.018873085
		\item -0.022110714
		\item -0.025317593
		\item -0.028491294
		\item -0.031631818
		\item -0.034736737
		\item -0.037805242
		\item -0.040835714
		\item -0.043826536
		\item -0.046777706
		\item -0.049685988
		\item -0.052552192
		\item -0.055373889
		\item -0.058149462
		\item -0.06087891
		\item -0.063559805
		\item -0.066192149
		\item -0.068774322
		\item -0.071305514
		\item -0.073784109
		\item -0.076209296
		\item -0.078580266
		\item -0.080895401
		\item -0.083154701
		\item -0.085356548
		\item -0.087500131
		\item -0.089585452
		\item -0.091610083
		\item -0.093574023
		\item -0.095476464
		\item -0.097316596
		\item -0.09909361
		\item -0.100807506
		\item -0.102455856
		\item -0.10403947
		\item -0.105557538
		\item -0.107008442
		\item -0.108392992
		\item -0.109709568
		\item -0.110958171
		\item -0.112137992
		\item -0.11324903
		\item -0.114290477
		\item -0.115261523
		\item -0.116162168
		\item -0.116992412
		\item -0.117751446
		\item -0.11843927
		\item -0.119055075
		\item -0.11959967
		\item -0.120071437
		\item -0.120471184
		\item -0.120798103
		\item -0.121053002
		\item -0.121235073
		\item -0.121344316
		\item -0.12138073
		\item -0.121344316
		\item -0.121235073
		\item -0.121053002
		\item -0.120798103
		\item -0.120471184
		\item -0.120071437
		\item -0.11959967
		\item -0.119055075
		\item -0.11843927
		\item -0.117751446
		\item -0.116992412
		\item -0.116162168
		\item -0.115261523
		\item -0.114290477
		\item -0.11324903
		\item -0.112137992
		\item -0.110958171
		\item -0.109709568
		\item -0.108392992
		\item -0.107008442
		\item -0.105557538
		\item -0.10403947
		\item -0.102455856
		\item -0.100807506
		\item -0.09909361
		\item -0.097316596
		\item -0.095476464
		\item -0.093574023
		\item -0.091610083
		\item -0.089585452
		\item -0.087500131
		\item -0.085356548
		\item -0.083154701
		\item -0.080895401
		\item -0.078580266
		\item -0.076209296
		\item -0.073784109
		\item -0.071305514
		\item -0.068774322
		\item -0.066192149
		\item -0.063559805
		\item -0.06087891
		\item -0.058149462
		\item -0.055373889
		\item -0.052552192
		\item -0.049685988
		\item -0.046777706
		\item -0.043826536
		\item -0.040835714
		\item -0.037805242
		\item -0.034736737
		\item -0.031631818
		\item -0.028491294
		\item -0.025317593
		\item -0.022110714
		\item -0.018873085
		\item -0.015606325
		\item -0.012311243
		\item -0.008989457
		\item -0.005642586
		\item -0.002273056
		\item 0.00111913
		\item 0.004532356
		\item 0.007963385
		\item 0.011412216
		\item 0.014876422
		\item 0.018355194
		\item 0.021846104
		\item 0.025347533
		\item 0.028857055
		\item 0.032374668
		\item 0.035897946
		\item 0.03942527
		\item 0.042954213
		\item 0.046483964
		\item 0.050012098
		\item 0.053536994
		\item 0.057056226
		\item 0.060569794
		\item 0.064073651
		\item 0.067567797
		\item 0.071049806
		\item 0.074517249
		\item 0.077968507
		\item 0.081402773
		\item 0.084815999
		\item 0.088208995
		\item 0.091577715
		\item 0.09492054
		\item 0.098236662
		\item 0.101523652
		\item 0.104779892
		\item 0.108002146
		\item 0.111189604
		\item 0.114340648
		\item 0.11745285
		\item 0.120523782
		\item 0.123551827
		\item 0.126535365
		\item 0.129472779
		\item 0.13236164
		\item 0.135199522
		\item 0.137984805
		\item 0.140715871
		\item 0.143391102
		\item 0.146008071
		\item 0.148564349
		\item 0.151058319
		\item 0.15348917
		\item 0.155853667
		\item 0.15815019
		\item 0.160376313
		\item 0.162531225
		\item 0.164613309
		\item 0.166619328
		\item 0.168547663
		\item 0.170397506
		\item 0.172165618
		\item 0.173851192
		\item 0.175451799
		\item 0.176965822
		\item 0.178391641
		\item 0.179726829
		\item 0.180970577
		\item 0.182120457
		\item 0.183174851
		\item 0.18413214
		\item 0.184989897
		\item 0.185747313
		\item 0.186402769
		\item 0.186953837
		\item 0.187399709
		\item 0.187737957
		\item 0.187966962
		\item 0.188086724
		\item 0.188093198
		\item 0.187987192
		\item 0.18776547
		\item 0.187427222
		\item 0.186970831
		\item 0.186395486
		\item 0.18569957
		\item 0.184880654
		\item 0.18393874
		\item 0.182872208
		\item 0.18167944
		\item 0.180359627
		\item 0.178910341
		\item 0.177332391
		\item 0.175622542
		\item 0.1737816
		\item 0.171807141
		\item 0.169698353
		\item 0.167454428
		\item 0.165075365
		\item 0.162557929
		\item 0.159903737
		\item 0.157110362
		\item 0.154177803
		\item 0.151104443
		\item 0.147890282
		\item 0.144535318
		\item 0.141037126
		\item 0.137396513
		\item 0.133612671
		\item 0.12968479
		\item 0.125612872
		\item 0.121396105
		\item 0.1170353
		\item 0.112528838
		\item 0.107876719
		\item 0.103078944
		\item 0.09813632
		\item 0.09304804
		\item 0.087813294
		\item 0.0824337
		\item 0.076908449
		\item 0.071237541
		\item 0.065422595
		\item 0.059461992
		\item 0.05335816
		\item 0.04710948
		\item 0.04071838
		\item 0.03418405
		\item 0.02750811
		\item 0.020690559
		\item 0.013732207
		\item 0.006634671
		\item -0.000601239
		\item -0.007975523
		\item -0.015486563
		\item -0.02313274
		\item -0.030913244
		\item -0.038826459
		\item -0.046870764
		\item -0.055045352
		\item -0.063348603
		\item -0.071778899
		\item -0.080333813
		\item -0.089012535
		\item -0.097812638
		\item -0.106731694
		\item -0.115768085
		\item -0.124919383
		\item -0.134183969
		\item -0.143559417
		\item -0.153042489
		\item -0.162630757
		\item -0.172322604
		\item -0.182114792
		\item -0.192004085
		\item -0.201987245
		\item -0.212062655
		\item -0.222226268
		\item -0.232474848
		\item -0.242805157
		\item -0.253213959
		\item -0.263697208
		\item -0.274251668
		\item -0.284873291
		\item -0.295558841
		\item -0.306303463
		\item -0.317103111
		\item -0.327954549
		\item -0.33885292
		\item -0.34979337
		\item -0.360771043
		\item -0.371782703
		\item -0.382822685
		\item -0.393885324
		\item -0.404966576
		\item -0.416061584
		\item -0.427163875
		\item -0.438268593
		\item -0.449370884
		\item -0.460463464
		\item -0.471541479
		\item -0.482599263
		\item -0.493630344
		\item -0.504628247
		\item -0.515587309
		\item -0.526500246
		\item -0.537360584
		\item -0.548161851
		\item -0.558897572
		\item -0.569559655
		\item -0.580140818
		\item -0.590634587
		\item -0.601033678
		\item -0.611329192
		\item -0.621513844
		\item -0.631579544
		\item -0.641519007
		\item -0.651323333
		\item -0.660983621
		\item -0.670491778
		\item -0.679839713
		\item -0.689016905
		\item -0.698015263
		\item -0.706825886
		\item -0.715439062
		\item -0.723845083
		\item -0.732033427
		\item -0.739996003
		\item -0.747721481
		\item -0.755199344
		\item -0.762419879
		\item -0.769371758
		\item -0.77604527
		\item -0.782428278
		\item -0.788510262
		\item -0.794279893
		\item -0.799725841
		\item -0.80483597
		\item -0.809598141
		\item -0.814001024
		\item -0.818030865
		\item -0.821677142
		\item -0.824926099
		\item -0.827765599
		\item -0.830181885
		\item -0.8321612
		\item -0.833691406
		\item -0.834757939
		\item -0.83534704
		\item -0.835444953
		\item -0.835037114
		\item -0.834108956
		\item -0.832646723
		\item -0.83063423
		\item -0.828056913
		\item -0.824899396
		\item -0.821145494
		\item -0.816780643
		\item -0.811788658
		\item -0.806151737
		\item -0.799855314
		\item -0.792882396
		\item -0.78521518
		\item -0.776836673
		\item -0.767729881
		\item -0.757877812
		\item -0.747261853
		\item -0.735864202
		\item -0.723665439
		\item -0.710649379
		\item -0.696794982
		\item -0.682084447
		\item -0.666498352
		\item -0.650016467
		\item -0.632620181
		\item -0.614288454
		\item -0.595001056
		\item -0.574738566
		\item -0.553479136
		\item -0.531201726
		\item -0.507885297
		\item -0.483508809
		\item -0.458049606
		\item -0.431485838
		\item -0.403794847
		\item -0.374954786
		\item -0.344942186
		\item -0.313734391
		\item -0.281307125
		\item -0.247637729
		\item -0.212701118
		\item -0.176473016
		\item -0.138929956
		\item -0.100046044
		\item -0.059797003
		\item -0.018156939
		\item 0.024900852
		\item 0.069400646
		\item 0.115369147
		\item 0.162833868
		\item 0.211821512
		\item 0.262357974
		\item 0.314472386
		\item 0.368191451
		\item 0.423544301
		\item 0.480557639
		\item 0.539262216
		\item 0.599686352
		\item 0.661858371
		\item 0.725809023
		\item 0.791568247
		\item 0.859165985
		\item 0.928632986
		\item 1
	\end{enumerate}
\end{multicols}
\subsubsection{Datos}
\[V1=
\begin{bmatrix}
1\ 16\ 10\ 1
\end{bmatrix}\]

\[V2=
\begin{bmatrix}
3\ 2\ 1
\end{bmatrix}\]
epochmax = 10000\\
alpha=.01\\
Múltiplo para las épocas de validación = 500\\
numval = 7\\
error de validación = .0000000000000001\\
Configuración: 80-10-10
\subsection{Resultado}
\subsection{Imágenes}
\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{pol3/1}
	\caption{Gráfica 3.1}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{pol3/2}
	\caption{Gráfica 3.2}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{pol3/3}
	\caption{Gráfica 3.3}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{pol3/4}
	\caption{Gráfica 3.4}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{pol3/5}
	\caption{Gráfica 3.5}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{pol3/6}
	\caption{Gráfica 3.6}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{pol3/7}
	\caption{Gráfica 3.7}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{pol3/8}
	\caption{Gráfica 3.8}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{pol3/9}
	\caption{Gráfica 3.9}
\end{figure}
\newpage

\subsection{Polinomio MLP}
\subsection{Inputs}
\begin{multicols}{4}
	\begin{enumerate}
		\item -1.0000
		\item -0.9940
		\item -0.9880
		\item -0.9820
		\item -0.9760
		\item -0.9700
		\item -0.9640
		\item -0.9580
		\item -0.9520
		\item -0.9460
		\item -0.9400
		\item -0.9340
		\item -0.9280
		\item -0.9220
		\item -0.9160
		\item -0.9100
		\item -0.9040
		\item -0.8980
		\item -0.8920
		\item -0.8860
		\item -0.8800
		\item -0.8740
		\item -0.8680
		\item -0.8620
		\item -0.8560
		\item -0.8500
		\item -0.8440
		\item -0.8380
		\item -0.8320
		\item -0.8260
		\item -0.8200
		\item -0.8140
		\item -0.8080
		\item -0.8020
		\item -0.7960
		\item -0.7900
		\item -0.7840
		\item -0.7780
		\item -0.7720
		\item -0.7660
		\item -0.7600
		\item -0.7540
		\item -0.7480
		\item -0.7420
		\item -0.7360
		\item -0.7300
		\item -0.7240
		\item -0.7180
		\item -0.7120
		\item -0.7060
		\item -0.7000
		\item -0.6940
		\item -0.6880
		\item -0.6820
		\item -0.6760
		\item -0.6700
		\item -0.6640
		\item -0.6580
		\item -0.6520
		\item -0.6460
		\item -0.6400
		\item -0.6340
		\item -0.6280
		\item -0.6220
		\item -0.6160
		\item -0.6100
		\item -0.6040
		\item -0.5980
		\item -0.5920
		\item -0.5860
		\item -0.5800
		\item -0.5740
		\item -0.5680
		\item -0.5620
		\item -0.5560
		\item -0.5500
		\item -0.5440
		\item -0.5380
		\item -0.5320
		\item -0.5260
		\item -0.5200
		\item -0.5140
		\item -0.5080
		\item -0.5020
		\item -0.4960
		\item -0.4900
		\item -0.4840
		\item -0.4780
		\item -0.4720
		\item -0.4660
		\item -0.4600
		\item -0.4540
		\item -0.4480
		\item -0.4420
		\item -0.4360
		\item -0.4300
		\item -0.4240
		\item -0.4180
		\item -0.4120
		\item -0.4060
		\item -0.4000
		\item -0.3940
		\item -0.3880
		\item -0.3820
		\item -0.3760
		\item -0.3700
		\item -0.3640
		\item -0.3580
		\item -0.3520
		\item -0.3460
		\item -0.3400
		\item -0.3340
		\item -0.3280
		\item -0.3220
		\item -0.3160
		\item -0.3100
		\item -0.3040
		\item -0.2980
		\item -0.2920
		\item -0.2860
		\item -0.2800
		\item -0.2740
		\item -0.2680
		\item -0.2620
		\item -0.2560
		\item -0.2500
		\item -0.2440
		\item -0.2380
		\item -0.2320
		\item -0.2260
		\item -0.2200
		\item -0.2140
		\item -0.2080
		\item -0.2020
		\item -0.1960
		\item -0.1900
		\item -0.1840
		\item -0.1780
		\item -0.1720
		\item -0.1660
		\item -0.1600
		\item -0.1540
		\item -0.1480
		\item -0.1420
		\item -0.1360
		\item -0.1300
		\item -0.1240
		\item -0.1180
		\item -0.1120
		\item -0.1060
		\item -0.1000
		\item -0.0940
		\item -0.0880
		\item -0.0820
		\item -0.0760
		\item -0.0700
		\item -0.0640
		\item -0.0580
		\item -0.0520
		\item -0.0460
		\item -0.0400
		\item -0.0340
		\item -0.0280
		\item -0.0220
		\item -0.0160
		\item -0.0100
		\item -0.0040
		\item  0.0020
		\item  0.0080
		\item  0.0140
		\item  0.0200
		\item  0.0260
		\item  0.0320
		\item  0.0380
		\item  0.0440
		\item  0.0500
		\item  0.0560
		\item  0.0620
		\item  0.0680
		\item  0.0740
		\item  0.0800
		\item  0.0860
		\item  0.0920
		\item  0.0980
		\item  0.1040
		\item  0.1100
		\item  0.1160
		\item  0.1220
		\item  0.1280
		\item  0.1340
		\item  0.1400
		\item  0.1460
		\item  0.1520
		\item  0.1580
		\item  0.1640
		\item  0.1700
		\item  0.1760
		\item  0.1820
		\item  0.1880
		\item  0.1940
		\item  0.2000
		\item  0.2060
		\item  0.2120
		\item  0.2180
		\item  0.2240
		\item  0.2300
		\item  0.2360
		\item  0.2420
		\item  0.2480
		\item  0.2540
		\item  0.2600
		\item  0.2660
		\item  0.2720
		\item  0.2780
		\item  0.2840
		\item  0.2900
		\item  0.2960
		\item  0.3020
		\item  0.3080
		\item  0.3140
		\item  0.3200
		\item  0.3260
		\item  0.3320
		\item  0.3380
		\item  0.3440
		\item  0.3500
		\item  0.3560
		\item  0.3620
		\item  0.3680
		\item  0.3740
		\item  0.3800
		\item  0.3860
		\item  0.3920
		\item  0.3980
		\item  0.4040
		\item  0.4100
		\item  0.4160
		\item  0.4220
		\item  0.4280
		\item  0.4340
		\item  0.4400
		\item  0.4460
		\item  0.4520
		\item  0.4580
		\item  0.4640
		\item  0.4700
		\item  0.4760
		\item  0.4820
		\item  0.4880
		\item  0.4940
		\item  0.5000
		\item  0.5060
		\item  0.5120
		\item  0.5180
		\item  0.5240
		\item  0.5300
		\item  0.5360
		\item  0.5420
		\item  0.5480
		\item  0.5540
		\item  0.5600
		\item  0.5660
		\item  0.5720
		\item  0.5780
		\item  0.5840
		\item  0.5900
		\item  0.5960
		\item  0.6020
		\item  0.6080
		\item  0.6140
		\item  0.6200
		\item  0.6260
		\item  0.6320
		\item  0.6380
		\item  0.6440
		\item  0.6500
		\item  0.6560
		\item  0.6620
		\item  0.6680
		\item  0.6740
		\item  0.6800
		\item  0.6860
		\item  0.6920
		\item  0.6980
		\item  0.7040
		\item  0.7100
		\item  0.7160
		\item  0.7220
		\item  0.7280
		\item  0.7340
		\item  0.7400
		\item  0.7460
		\item  0.7520
		\item  0.7580
		\item  0.7640
		\item  0.7700
		\item  0.7760
		\item  0.7820
		\item  0.7880
		\item  0.7940
		\item  0.8000
		\item  0.8060
		\item  0.8120
		\item  0.8180
		\item  0.8240
		\item  0.8300
		\item  0.8360
		\item  0.8420
		\item  0.8480
		\item  0.8540
		\item  0.8600
		\item  0.8660
		\item  0.8720
		\item  0.8780
		\item  0.8840
		\item  0.8900
		\item  0.8960
		\item  0.9020
		\item  0.9080
		\item  0.9140
		\item  0.9200
		\item  0.9260
		\item  0.9320
		\item  0.9380
		\item  0.9440
		\item  0.9500
		\item  0.9560
		\item  0.9620
		\item  0.9680
		\item  0.9740
		\item  0.9800
		\item  0.9860
		\item  0.9920
		\item  0.9980
	\end{enumerate}
\end{multicols}
\subsection{Targets}
\begin{multicols}{4}
	\begin{enumerate}
		   \item 0.0000
		\item  0.6690
		\item  1.3135
		\item  1.9257
		\item  2.4983
		\item  3.0250
		\item  3.4999
		\item  3.9181
		\item  4.2757
		\item  4.5694
		\item  4.7970
		\item  4.9572
		\item  5.0496
		\item  5.0748
		\item  5.0339
		\item  4.9294
		\item  4.7642
		\item  4.5420
		\item  4.2672
		\item  3.9447
		\item  3.5801
		\item  3.1792
		\item  2.7482
		\item  2.2935
		\item  1.8218
		\item  1.3396
		\item  0.8536
		\item  0.3702
		\item -0.1044
		\item -0.5641
		\item -1.0033
		\item -1.4169
		\item -1.8003
		\item -2.1491
		\item -2.4599
		\item -2.7297
		\item -2.9561
		\item -3.1375
		\item -3.2728
		\item -3.3616
		\item -3.4042
		\item -3.4014
		\item -3.3546
		\item -3.2659
		\item -3.1377
		\item -2.9729
		\item -2.7749
		\item -2.5473
		\item -2.2942
		\item -2.0197
		\item -1.7281
		\item -1.4239
		\item -1.1115
		\item -0.7954
		\item -0.4799
		\item -0.1691
		\item  0.1329
		\item  0.4225
		\item  0.6962
		\item  0.9509
		\item  1.1840
		\item  1.3930
		\item  1.5760
		\item  1.7316
		\item  1.8586
		\item  1.9564
		\item  2.0249
		\item  2.0641
		\item  2.0748
		\item  2.0578
		\item  2.0144
		\item  1.9463
		\item  1.8555
		\item  1.7439
		\item  1.6140
		\item  1.4684
		\item  1.3095
		\item  1.1403
		\item  0.9633
		\item  0.7814
		\item  0.5972
		\item  0.4135
		\item  0.2326
		\item  0.0570
		\item -0.1112
		\item -0.2699
		\item -0.4175
		\item -0.5523
		\item -0.6732
		\item -0.7790
		\item -0.8691
		\item -0.9429
		\item -1.0002
		\item -1.0411
		\item -1.0657
		\item -1.0745
		\item -1.0684
		\item -1.0480
		\item -1.0145
		\item -0.9691
		\item -0.9130
		\item -0.8477
		\item -0.7746
		\item -0.6952
		\item -0.6110
		\item -0.5236
		\item -0.4344
		\item -0.3448
		\item -0.2562
		\item -0.1699
		\item -0.0869
		\item -0.0084
		\item  0.0648
		\item  0.1319
		\item  0.1923
		\item  0.2455
		\item  0.2912
		\item  0.3292
		\item  0.3595
		\item  0.3821
		\item  0.3972
		\item  0.4051
		\item  0.4064
		\item  0.4014
		\item  0.3907
		\item  0.3750
		\item  0.3549
		\item  0.3312
		\item  0.3045
		\item  0.2756
		\item  0.2452
		\item  0.2139
		\item  0.1824
		\item  0.1513
		\item  0.1211
		\item  0.0922
		\item  0.0652
		\item  0.0403
		\item  0.0178
		\item -0.0021
		\item -0.0193
		\item -0.0337
		\item -0.0453
		\item -0.0542
		\item -0.0606
		\item -0.0646
		\item -0.0665
		\item -0.0663
		\item -0.0645
		\item -0.0614
		\item -0.0571
		\item -0.0519
		\item -0.0463
		\item -0.0403
		\item -0.0343
		\item -0.0285
		\item -0.0230
		\item -0.0179
		\item -0.0135
		\item -0.0097
		\item -0.0066
		\item -0.0041
		\item -0.0024
		\item -0.0012
		\item -0.0005
		\item -0.0001
		\item -0.0000
		\item  0.0000
		\item  0.0001
		\item  0.0003
		\item  0.0009
		\item  0.0019
		\item  0.0035
		\item  0.0057
		\item  0.0086
		\item  0.0121
		\item  0.0164
		\item  0.0212
		\item  0.0266
		\item  0.0323
		\item  0.0383
		\item  0.0443
		\item  0.0501
		\item  0.0554
		\item  0.0600
		\item  0.0636
		\item  0.0659
		\item  0.0666
		\item  0.0655
		\item  0.0622
		\item  0.0567
		\item  0.0486
		\item  0.0378
		\item  0.0244
		\item  0.0081
		\item -0.0109
		\item -0.0325
		\item -0.0566
		\item -0.0830
		\item -0.1113
		\item -0.1411
		\item -0.1720
		\item -0.2034
		\item -0.2348
		\item -0.2656
		\item -0.2951
		\item -0.3226
		\item -0.3474
		\item -0.3688
		\item -0.3860
		\item -0.3984
		\item -0.4054
		\item -0.4063
		\item -0.4006
		\item -0.3879
		\item -0.3678
		\item -0.3402
		\item -0.3047
		\item -0.2616
		\item -0.2108
		\item -0.1528
		\item -0.0879
		\item -0.0166
		\item  0.0602
		\item  0.1418
		\item  0.2271
		\item  0.3151
		\item  0.4045
		\item  0.4940
		\item  0.5822
		\item  0.6676
		\item  0.7487
		\item  0.8241
		\item  0.8922
		\item  0.9515
		\item  1.0006
		\item  1.0383
		\item  1.0631
		\item  1.0741
		\item  1.0703
		\item  1.0510
		\item  1.0157
		\item  0.9639
		\item  0.8955
		\item  0.8108
		\item  0.7102
		\item  0.5942
		\item  0.4639
		\item  0.3205
		\item  0.1652
		\item  0.0000
		\item -0.1734
		\item -0.3527
		\item -0.5358
		\item -0.7201
		\item -0.9031
		\item -1.0820
		\item -1.2541
		\item -1.4168
		\item -1.5671
		\item -1.7025
		\item -1.8205
		\item -1.9185
		\item -1.9944
		\item -2.0462
		\item -2.0721
		\item -2.0708
		\item -2.0412
		\item -1.9825
		\item -1.8945
		\item -1.7771
		\item -1.6310
		\item -1.4569
		\item -1.2564
		\item -1.0311
		\item -0.7834
		\item -0.5157
		\item -0.2310
		\item  0.0673
		\item  0.3756
		\item  0.6900
		\item  1.0064
		\item  1.3205
		\item  1.6279
		\item  1.9241
		\item  2.2048
		\item  2.4656
		\item  2.7021
		\item  2.9104
		\item  3.0866
		\item  3.2274
		\item  3.3296
		\item  3.3906
		\item  3.4083
		\item  3.3809
		\item  3.3076
		\item  3.1878
		\item  3.0217
		\item  2.8101
		\item  2.5545
		\item  2.2571
		\item  1.9206
		\item  1.5483
		\item  1.1443
		\item  0.7130
		\item  0.2595
		\item -0.2107
		\item -0.6918
		\item -1.1777
		\item -1.6619
		\item -2.1378
		\item -2.5989
		\item -3.0385
		\item -3.4502
		\item -3.8276
		\item -4.1647
		\item -4.4560
		\item -4.6963
		\item -4.8809
		\item -5.0060
		\item -5.0684
		\item -5.0654
		\item -4.9956
		\item -4.8580
		\item -4.6527
		\item -4.3808
		\item -4.0442
		\item -3.6458
		\item -3.1893
		\item -2.6793
		\item -2.1213
		\item -1.5215
		\item -0.8870
		\item -0.2252
	\end{enumerate}
\end{multicols}
\subsubsection{Datos}
\[V1=
\begin{bmatrix}
1\ 10\ 12\ 1
\end{bmatrix}\]

\[V2=
\begin{bmatrix}
2\ 3\ 1
\end{bmatrix}\]
epochmax = 43200\\
alpha=.003\\
Múltiplo para las épocas de validación = 75\\
numval = 7\\
error de validación = .000000000001\\
Configuración: 80-10-10
\subsection{Resultado}
\subsection{Imágenes}
\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{mlp/1}
	\caption{Gráfica 5.1}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{mlp/2}
	\caption{Gráfica 5.2}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{mlp/3}
	\caption{Gráfica 5.3}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{mlp/4}
	\caption{Gráfica 5.4}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{mlp/5}
	\caption{Gráfica 5.5}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{mlp/6}
	\caption{Gráfica 5.6}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{mlp/7}
	\caption{Gráfica 5.7}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{mlp/8}
	\caption{Gráfica 5.8}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width = 500pt, height = 500pt]{mlp/9}
	\caption{Gráfica 5.9}
\end{figure}
\newpage

\section{Discusión de Resultados}
Para cada uno de los resultados se muestra:
\begin{enumerate}
	\item Los datos con los cuales fue realizado el ejemplo.
	\item Los pesos y bias iniciales.
	\item El ``historial'' para cada capa.
	\item Los resultados de la red vs los targets del dataset de entrenamiento.
	\item Los resultados de la red vs los targets del dataset de prueba.
\end{enumerate}
\section{Conclusiones}
El arte de diseñar y entrenar redes neuronales radica en las pruebas con distintas arquitecturas/configuraciones para resolver un problema en específico, realizar la implementación del programa fue bastante complicado, ya que todos en nuestro equipo teníamos diferentes ideas a implementar, pero al final con un sistema de votos se solucionó, aprendimos a usar el tipo de dato cell, para prealocar espacio. El MLP es la base de las redes neuronales artificales modernas, este sigue siendo usado para resolver muchos problemas, pudimos observar como se comporta con diferentes arquitecturas, funciones de transferencia y configuraciones, una práctica muy interesante y muy difícil de probar.
\section{Referencias}
[1] Martin T Hagan. Machine Learning, Neural Network Design (2nd Edition), 2014.\newline
[2] ``Perceptrón Multicapa'', notas de clase de Neural Networks, Department of Engineering in Computer Systems, Escuela Superior de Cómputo, 2017.\newline
[3] Math Works, “MATLAB”. Disponible en: \url{https://es.mathworks.com/products/matlab}.

\section{Apéndice}
\begin{lstlisting}[
style=Matlab-editor,
basicstyle=\mlttfamily,
escapechar=`,
caption={mlp.m},
]
clc
clear

% Read the inputs file
inputs_path = strcat(input('Ingrese el nombre del archivo de inputs sin la extensión: ','s'), '.txt');
%inputs_path = 'inputs.txt';
inputs = importdata(inputs_path);

% Read the targets
targets_path = strcat(input('Ingrese el nombre del archivo de targets sin la extensión: ','s'), '.txt');
%targets_path = 'targets.txt';
targets = importdata(targets_path);

data_size = size(inputs, 1);

% Enter MLP architecture
architecture = str2num(input('Ingrese el vector de la arquitectura: ','s'));
% Calculate layer parameters
%architecture = str2num('1 16 10 1');
num_layers = length(architecture) - 1;
R = architecture(1);
functions_vector = str2num(input('Ingrese el vector de las funciones de activación: 1) purelin()\n2) logsig()\n3) tansig()\n\n: ','s'));
%functions_vector = str2num('3 2 1');

% Enter the learning factor
alpha = input('Ingresa el valor del factor de aprendizaje(alpha): ');
%alpha = .01;

epochmax = input('Ingresa el número máximo de épocas: ');
% epochmax = 10000;
%validation_iter = 500;
%numval = 7;
%error_epoch_validation = .0000000000000001;
numval = input('Numero maximo de incrementos consecutivos del error de validacion (numval): ');
error_epoch_validation = input('Ingrese el valor minimo del error de epoca (error_epoch_validation): ');
validation_iter = input('Ingrese el múltiplo de épocas para realizar una época de validación  (validation_iter): ');

% Dataset Slicing
config_option = input('Elija una configuración de distribución de datasets: \n1: 80-10-10\n2: 70-15-15\n');
%config_option = 2;
[training_ds, test_ds, validation_ds] = dataset_slices(config_option, inputs, targets);
validation_ds_size = size(validation_ds, 1);
test_ds_size = size(test_ds, 1);
training_ds_size = size(training_ds, 1);

disp('Dataset de entrenamiento:');
disp(training_ds);
disp('Dataset de validacion:');
disp(validation_ds);
disp('Dataset de prueba:');
disp(test_ds);

% Open the files for weights and bias
total_weight_files = 0;
total_bias_files = 0;
for i=1:num_layers
% For neurons
for j=1:architecture(i + 1)
% For weights
for l=1:architecture(i)
total_weight_files = total_weight_files + 1;
end
end
total_bias_files = total_bias_files + 1;
end

W_files = zeros(total_weight_files, 1);
b_files = zeros(total_bias_files, 1);

current_file = 1;
for i=1:num_layers
path = strcat(pwd, '/historico/capa_', num2str(i), '/pesos/');
if ~exist(path, 'dir')
mkdir(path);
end
% For layers
for j=1:architecture(i + 1)
% For neurons
for k=1:architecture(i)
archivo_pesos = strcat(path, '/pesos', num2str(j), '_', num2str(k),'.txt');
W_files(current_file) = fopen(archivo_pesos,'w');
current_file = current_file +1;
end
end
end

current_file = 1;
for i=1:num_layers
path = strcat(pwd,'/historico/capa_', num2str(i), '/bias/');
if ~exist(path, 'dir')
mkdir(path);
end
for j=1:architecture(i+1)
archivo_bias = strcat(path,'/bias',num2str(j),'.txt');
b_files(current_file) = fopen(archivo_bias,'w');
current_file = current_file +1;
end
end

% Initialize MLP parameters and Print them

num_w_files = 1;
num_b_files = 1;
W = cell(num_layers,1);
b = cell(num_layers,1);
% Output of each layer
a = cell(num_layers + 1, 1);
% Sentitivities
S = cell(num_layers, 1);
% Derivatives of each layer
F_m = cell(num_layers, 1);

% For each layer
for i=1:num_layers
% Random value
W_r_value = 2 * rand(architecture(i + 1), architecture(i)) - 1;
b_r_value = 2* rand(architecture(i + 1), 1) - i;
W{i} = W_r_value
b{i} = b_r_value
% For each neuron
for j=1:architecture(i + 1)
%For each weight
for k=1:architecture(i)
% Print wights value
fprintf(W_files(num_w_files), '%f\r\n', W_r_value(j, k));
num_w_files = num_w_files + 1;
end
end
% For each neuron
for j=1:architecture(i + 1)
% print bias value
fprintf(b_files(num_b_files), '%f\r\n', b_r_value(j));
num_b_files = num_b_files + 1;
end
end




% Learning algorithm
num_validation_epoch = 0;
early_stopping_increment = 0;
validation_error = 0;
learning_error = 0;
early_s_counter = 0;

% initialize vectors for printing errors
learning_err_values = zeros(epochmax, 1);
evaluation_err_values = zeros(ceil(epochmax / validation_iter), 1);
for epoch=1:epochmax
l_error = 0;
% Reset the values
num_w_files = 1;
num_b_files = 1;
% if isn't a validation epoch
if(mod(epoch ,validation_iter) ~= 0)
for t_data=1:training_ds_size    
% initial condition
a{1} = training_ds(t_data, 1); 
% Foward propagation
for t_p=1:num_layers
W_aux = cell2mat(W(t_p));
b_aux = cell2mat(b(t_p));
a_aux = cell2mat(a(t_p));
n_f = W_aux * a_aux + b_aux;
a{t_p + 1} = get_activation_function(n_f, functions_vector(t_p));
end
a_aux = cell2mat(a(num_layers + 1));
t_error = training_ds(t_data, 2) - a_aux;
l_error = l_error + (t_error / data_size);
% Sensitivities calculation
F_m{num_layers} = get_F_matrix(functions_vector(num_layers), architecture(num_layers + 1), a_aux);
F_m_temp = cell2mat(F_m(num_layers));
S{num_layers} = F_m_temp * (t_error)*(-2);
% Backpropagation
for m = num_layers-1:-1:1
W_aux = cell2mat(W(m+1));
s_aux = cell2mat(S(m+1));
a_aux = cell2mat(a(m+1));
F_m{m} = get_F_matrix(functions_vector(m),architecture(m+1),a_aux);
F_m_temp = cell2mat(F_m(m));
S{m} = F_m_temp * (W_aux')*s_aux;
end
% Learning Rules
for k = num_layers:-1:1
W_aux = cell2mat(W(k));
b_aux = cell2mat(b(k));
s_aux = cell2mat(S(k));
a_aux = cell2mat(a(k));
W{k} = W_aux - (alpha * s_aux * a_aux');
b{k} = b_aux - (alpha * s_aux);
W_aux = cell2mat(W(k));
b_aux = cell2mat(b(k));
end
end
learning_error = l_error;
learning_err_values(epoch) = l_error;      
% This epoch is a validation one
else
val_error = 0;
num_validation_epoch = num_validation_epoch + 1;
for t_data = 1:validation_ds_size
% Initial Condition
a{1} = validation_ds(t_data, 1);
% Foward propagation
for k=1:num_layers
W_aux = cell2mat(W(k));
a_aux = cell2mat(a(k));
b_aux = cell2mat(b(k));
n_f = W_aux * a_aux + b_aux;
a{k + 1} = get_activation_function(n_f, functions_vector(k));
end
a_aux = cell2mat(a(num_layers+1));
val_error = validation_ds(t_data,2)-a_aux;
val_error = val_error+(val_error/validation_ds_size);
end
evaluation_err_values(epoch) = val_error;
if early_stopping_increment == 0
validation_error = val_error;
early_stopping_increment = early_stopping_increment+1;
fprintf('Incremento actual para early stopping = %d\n', early_stopping_increment);
else
if val_error > validation_error
validation_error = val_error;
early_stopping_increment = early_stopping_increment+1;
fprintf('Incremento actual para early stopping = %d\n', early_stopping_increment);
if early_stopping_increment == numval
% Reset the counter
early_s_counter = 1;
fprintf('Early stopping en la época:  %d\n', epoch);
break;
end
else
validation_error = 0;
early_stopping_increment = 0;
fprintf('Incremento actual para early stopping = %d\n', early_stopping_increment);
end
end
end

% Print the values on console
num_w_files = 1;
num_b_files = 1;
for k = num_layers:-1:1
W_aux = cell2mat(W(k));
b_aux = cell2mat(b(k));
for j=1:architecture(k+1)
for l=1:architecture(k)
fprintf(W_files(num_w_files), '%f\r\n', W_aux(j,l));
num_w_files = num_w_files +1;
end
end
for j=1:architecture(k + 1)
fprintf(b_files(num_b_files), '%f\r\n', b_aux(j));
num_b_files = num_b_files + 1;
end
end

% Check stopping calculations
if mod(epoch,validation_iter) ~= 0 && l_error <= error_epoch_validation && l_error >= 0
learning_error = l_error;
fprintf('Aprendizaje exitoso en la época %d\n', epoch);
break;
end
end

if epoch == epochmax
disp('Se llego a epochmax');
end

% Print the las final values 
if early_s_counter == 1
num_w_files = 1;
num_b_files = 1;
for k = num_layers:-1:1
W_aux = cell2mat(W(k));
b_aux = cell2mat(b(k));
for j = 1:architecture(k + 1)
for l=1:architecture(k)
fprintf(W_files(num_w_files), '%f\r\n', W_aux(j, l));
num_w_files = num_w_files + 1;
end
end
for j=1:architecture(k + 1)
fprintf(b_files(num_b_files), '%f\r\n', b_aux(j));
num_b_files = num_b_files + 1;
end
end
end

% Close all files
for i=1:total_weight_files
fclose(W_files(i));
end
for i=1:total_bias_files
fclose(b_files(i));
end

% Propagate the test dataset
test_error = 0;
output = zeros(test_ds_size,1);
for i=1:test_ds_size
% Initial condition
a{1} = test_ds(i,1);
for k=1:num_layers
W_aux = cell2mat(W(k));
a_aux = cell2mat(a(k));
b_aux = cell2mat(b(k));
n_f = W_aux*a_aux+b_aux;
a{k+1} = get_activation_function(n_f, functions_vector(k));
end
test_data = cell2mat(a(1));
a_aux = cell2mat(a(num_layers + 1));
test_error = test_error + (1 / test_ds_size) * (test_ds(i,2) - a_aux);
output(i) = a_aux;
end

% Print last errors
fprintf('Error de aprendizaje = %f\n', learning_error);
fprintf('Error de validación = %f\n', validation_error);
fprintf('Error de prueba = %f\n', test_error);


% Output vs test
scatter_output_vs_test(test_ds, output);
% Propagate the training size for ploting

output = zeros(training_ds_size,1);
for i=1:training_ds_size
% Initial Condition
a{1} = training_ds(i, 1);
for k=1:num_layers
W_aux = cell2mat(W(k));
a_aux = cell2mat(a(k));
b_aux = cell2mat(b(k));
a{k+1} = get_activation_function(W_aux*a_aux+b_aux,functions_vector(k));
end
a_aux = cell2mat(a(num_layers + 1));
test_error = test_error + (1 / training_ds_size) * (training_ds(i,2) - a_aux);
output(i) = a_aux;
end

scatter_output_vs_training(training_ds, output);

% Plot the error evolution
error_plot(validation_iter, num_validation_epoch, learning_err_values, epoch, evaluation_err_values);
% Plot weight evolution
weight_evolution_plot(architecture, num_layers, epoch);
% Plot bias evolution
bias_evolution_plot(architecture, num_layers, epoch);

% Write final values
for i=1:num_layers
path = strcat(pwd, '/Valores_finales/capa_', num2str(i), '/');
if ~exist(path, 'dir')
mkdir(path);
end
W_aux = cell2mat(W(i));
res_pesos = strcat(path, '/pesos.txt');
dlmwrite(res_pesos, W_aux, ';');
end

for i=1:num_layers
path = strcat(pwd,'/Valores_finales/capa_', num2str(i), '/');
if ~exist(path, 'dir')
mkdir(path);
end
b_aux = cell2mat(b(i));
res_bias = strcat(path, '/bias.txt');
dlmwrite(res_bias, b_aux, ';');
end
\end{lstlisting}

\begin{lstlisting}[
style=Matlab-editor,
basicstyle=\mlttfamily,
escapechar=`,
caption={bias\_evolution\_plot.m},
]
function bias_evolution_plot(architecture, num_layers, epoch)
num_epochs = 0:1:epoch;
for i=1:num_layers
figure('Name', sprintf('Evolución de bias capa %d', i), 'NumberTitle', 'off')
path = strcat(pwd, '/historico/capa_', num2str(i), '/bias/');
for j=1:architecture(i+1)
file = strcat(path,'/bias',num2str(j),'.txt');
identifier = strcat('b(',num2str(j),')');
evolution = importdata(file);
plot(num_epochs, evolution', 'DisplayName', identifier);
hold on
grid on
end
title(sprintf('Evolución de bias capa %d', i));
ylabel('Valor');
xlabel('Epoch');
title(legend('show', 'Location', 'northwestoutside'), 'Leyenda');
hold off
end
end
\end{lstlisting}

\begin{lstlisting}[
style=Matlab-editor,
basicstyle=\mlttfamily,
escapechar=`,
caption={dataset\_slices.m},
]
function [training_ds, test_ds, validation_ds] = dataset_slices (opcion, inputs, targets)
% Get the data size
num_data = size (inputs, 1)
% COnfiguration options
if opcion == 1
training_num = ceil (num_data * .8);
test_num = floor (num_data * 0.1 );
validation_num =  floor (num_data * 0.1);
else
training_num = ceil (num_data * 0.7)
test_num = floor (num_data * 0.15)
validation_num =  floor (num_data * 0.15)
end
% Initialize the datasets
training_ds = zeros (training_num, 2);
test_ds = zeros (test_num, 2);
validation_ds = zeros (validation_num, 2);
a1 = 2:ceil(num_data / validation_num):num_data-1
aux = setdiff(1:num_data, a1);
j = 2;
a2 = [];
increment = ceil(size(aux, 2) / test_num);
for i=1:test_num
if (j < size(aux, 2))
a2 = [a2, aux(j)];
else
a2 = [a2, aux(end - 1)];
end
j = j + increment;
end
a2
a3 = setdiff(aux, a2)

% Validation Slice
for i = 1:size(a1, 2)
validation_ds (i, 1) = inputs(a1(i));
validation_ds (i, 2) = targets(a1(i));
end    

% test Slice
for i = 1:size(a2, 2)
test_ds (i, 1) = inputs(a2(i));
test_ds (i, 2) = targets(a2(i));
end   


% training Slice
for i = 1:size(a3, 2)
training_ds(i, 1) = inputs(a3(i));
training_ds(i, 2) = targets(a3(i));
end  
end

\end{lstlisting}

\begin{lstlisting}[
style=Matlab-editor,
basicstyle=\mlttfamily,
escapechar=`,
caption={get\_activation\_function.m},
]
function a = get_activation_function(n, config)
if (config == 1)
a = purelin(n);
elseif (config == 2)
a = logsig(n);
else
a = tansig(n);
end
end
\end{lstlisting}


\begin{lstlisting}[
style=Matlab-editor,
basicstyle=\mlttfamily,
escapechar=`,
caption={get\_F\_matrix.m},
]
function F = get_F_matrix(option, total_neurons, a)
if (option == 1)
F = diag(ones(1, total_neurons));
elseif (option == 2)
F = diag(logsig('dn', a, a));
else
F = diag(tansig('dn', a, a));
end
end

\end{lstlisting}

\begin{lstlisting}[
style=Matlab-editor,
basicstyle=\mlttfamily,
escapechar=`,
caption={scatter\_output\_vs\_test.m},
]
function scatter_output_vs_test(test_ds, output)
% Targets vs MLP output PLOT
figure('Name','Targets del dataset de prueba vs Output', 'NumberTitle', 'off')
grid on
hold on
title('Targets del dataset de prueba vs Output');
ylabel('G(p)');
xlabel('rango de la señal');
signal_range = test_ds(:, 1);
% targets
scatter(signal_range,test_ds(:,2), 'd', 'MarkerEdgeColor', 'black');
% outputs
scatter(signal_range, output, 5, 'MarkerFaceColor', 'blue', 'MarkerEdgeColor', 'blue' ); 
title(legend('Target', 'Output', 'Location', 'northwestoutside'),'Leyenda');
hold off
end
\end{lstlisting}

\begin{lstlisting}[
style=Matlab-editor,
basicstyle=\mlttfamily,
escapechar=`,
caption={scatter\_output\_vs\_training.m},
]
function scatter_output_vs_training(training_ds, output)
% Targets vs MLP output PLOT
figure('Name','Targets del dataset de entrenamiento vs Output', 'NumberTitle', 'off')
grid on
hold on
title('Targets del dataset de entrenamiento vs Output');
ylabel('G(p)');
xlabel('rango de la señal');
signal_range = training_ds(:, 1);
% targets
scatter(signal_range, training_ds(:,2), 'd', 'MarkerEdgeColor', 'black');
% outputs
scatter(signal_range, output, 5, 'MarkerFaceColor', 'blue', 'MarkerEdgeColor', 'blue' ); 
title(legend('Target', 'Output', 'Location', 'northwestoutside'),'Leyenda');
hold off
end


\end{lstlisting}

\begin{lstlisting}[
style=Matlab-editor,
basicstyle=\mlttfamily,
escapechar=`,
caption={weight\_evolution\_plot.m},
]
function weight_evolution_plot(architecture, num_layers, epoch)
num_epochs = 0:1:epoch;
for i=1:num_layers
figure('Name',sprintf('Evolución de pesos capa %d', i), 'NumberTitle', 'off')
path = strcat(pwd, '/historico/capa_', num2str(i), '/pesos/');
for j = 1:architecture(i + 1)
for k = 1:architecture(i)
W_file = strcat(path, '/pesos',num2str(j), '_', num2str(k), '.txt');
identifier = strcat('W(',num2str(j),',',num2str(k),')');
evolution = importdata(W_file);
plot(num_epochs, evolution', 'DisplayName', identifier);
hold on
grid on
end
end
title(sprintf('Evolución de pesos capa %d', i));
xlabel('Epoch');
ylabel('Valor');
title(legend('show', 'Location', 'northwestoutside'), 'Leyenda');
hold off
end
end
\end{lstlisting}

\end{document}